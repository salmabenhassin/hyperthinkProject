import os
import uvicorn
import time
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List

# Imports internes
from src.config import Config
from src.contextual import ContextualProcessor
from src.vector_store import HybridStore
from src.models import ModelFactory

# Configuration
TARGET_PDF = "data/attention_is_all_you_need.pdf"
INDEX_FOLDER = "data/faiss_index"

app = FastAPI(title="RAG : Attention Is All You Need", version="2.0")

# Initialisation globale
rag_store = HybridStore()
llm_client = ModelFactory.get_llm()

# --- Mod√®les Pydantic ---
class QueryRequest(BaseModel):
    q: str
    k: int = 6

class SourceItem(BaseModel):
    chunk_id: int
    score: float
    method: str
    preview: str

class QueryResponse(BaseModel):
    answer: str
    sources: List[SourceItem]

# --- Logique d'Ingestion Automatique ---
# Dans main.py

def ingest_local_file():
    """
    Lit le fichier PDF local, le d√©coupe, g√©n√®re les embeddings et sauvegarde l'index.
    """
    print(f"üîÑ D√©marrage de l'ingestion pour : {TARGET_PDF}")
    
    if not os.path.exists(TARGET_PDF):
        print(f"‚ùå ERREUR CRITIQUE : Le fichier {TARGET_PDF} est introuvable !")
        return False

    processor = ContextualProcessor()
    
    # 1. Chargement et d√©coupage
    print("‚úÇÔ∏è Lecture et d√©coupage du PDF...")
    raw_docs, basic_chunks = processor.load_and_split(TARGET_PDF)
    
    # 2. Contextualisation (Attention au Rate Limit Cohere !)
    print(f"üß† G√©n√©ration des embeddings pour {len(basic_chunks)} segments...")
    
    # Pause de s√©curit√© pour √©viter l'erreur 429 si n√©cessaire
    # time.sleep(2) 
    
    contextualized_chunks = processor.generate_contextual_chunks(
        "Ce document pr√©sente l'architecture Transformer.", 
        basic_chunks
    )
    
    # 3. Construction et Sauvegarde
    print("üíæ Cr√©ation et sauvegarde de l'index FAISS...")
    
    # --- CORRECTION ICI : On ajoute le deuxi√®me argument (filename) ---
    rag_store.build_index(contextualized_chunks, os.path.basename(TARGET_PDF)) 
    # -----------------------------------------------------------------
    
    return True

# --- √âv√©nement de D√©marrage (Le C≈ìur du Syst√®me) ---
@app.on_event("startup")
async def startup_event():
    print("üöÄ Initialisation du serveur RAG...")
    
    # √âtape 1 : Essayer de charger l'index existant
    if rag_store.load_index():
        print("‚úÖ Index FAISS charg√© depuis le disque. Le syst√®me est pr√™t instantan√©ment !")
        # Petite v√©rification optionnelle : est-ce qu'il y a des docs ?
        if rag_store.vector_db:
            print(f"   -> Base vectorielle active.")
    
    # √âtape 2 : Si pas d'index, on le cr√©e
    else:
        print("‚ö†Ô∏è Aucun index trouv√©. Lancement de l'ingestion automatique...")
        success = ingest_local_file()
        if success:
             print("‚úÖ Ingestion termin√©e avec succ√®s. Le syst√®me est pr√™t.")
        else:
             print("‚ùå √âchec de l'initialisation. V√©rifiez que le fichier PDF est bien dans /data.")

# --- Endpoint de Question (Lecture Seule) ---
@app.post("/query", response_model=QueryResponse)
async def query_endpoint(request: QueryRequest):
    if not rag_store.vector_db:
        raise HTTPException(status_code=503, detail="Le syst√®me est en cours d'initialisation ou l'index est vide.")

    try:
        # 1. Recherche
        retrieved_docs = rag_store.search(request.q, k=request.k)
        
        # 2. Prompt Syst√®me Strict
# Dans main.py, √† l'int√©rieur de query_endpoint
        
        system_prompt = (
            "You are an expert on the research paper 'Attention Is All You Need'. "
            "Answer the user's question using ONLY the context provided below. "
            "Strict rules:\n"
            "1. Answer in English.\n"
            "2. If the answer is not in the context, say exactly: 'I cannot answer this based on the provided context.'\n"
            "3. Do not use outside knowledge.\n"
            "4. Always cite the source index (e.g., [Source 1])."
        )
        context_str = "\n\n".join([f"[Source {i+1}] {d['content']}" for i, d in enumerate(retrieved_docs)])

        # 3. G√©n√©ration
        response = llm_client.invoke([
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"CONTEXTE:\n{context_str}\n\nQUESTION:\n{request.q}"}
        ])

        # 4. Formatage
        sources_output = []
        for doc in retrieved_docs:
            c_id = doc.get("chunk_id", 0) # S√©curit√© int
            sources_output.append({
                "chunk_id": int(c_id) if c_id is not None else 0,
                "score": float(doc.get("score", 0.0)),
                "method": str(doc.get("source_method", "Unknown")),
                "preview": str(doc.get("content", ""))[:80] + "..."
            })

        return {"answer": response.content, "sources": sources_output}

    except Exception as e:
        print(f"Erreur Query: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)